{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "\n",
    "import holoviews as hv\n",
    "from colorcet import fire\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from holoviews.operation import gridmatrix\n",
    "from holoviews.operation.datashader import datashade\n",
    "from matplotlib.colors import LogNorm\n",
    "from ssc.cluster import selfrepresentation as sr\n",
    "import tqdm\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data and Select Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change working directory to where data is stored\n",
    "os.chdir(\"/home/idies/workspace/codex/Rare_cell_types/Thymus/gated_SPandVOL_Segmresults_merged\")\n",
    "os.getcwd()\n",
    "\n",
    "#Read in Dataset and Print Headers\n",
    "df = pd.read_csv('reg001_comp_SP_merged_real.tsv',sep='\\t', header=0)\n",
    "\n",
    "#Rename Columns\n",
    "df.columns = df.columns.str.split(':').str[-1].tolist()\n",
    "df.head()\n",
    "\n",
    "#Create dataframe to save X, Y columns\n",
    "df_loc = df.loc[:,['X','Y']]\n",
    "\n",
    "#Read in clustering dataframe\n",
    "os.chdir(\"/home/idies/workspace/codex/Rare_cell_types/Thymus/Xshift_clustering_results\")\n",
    "df_cluster = pd.read_csv('VOLreg1_K10_cl112.csv', header=0)\n",
    "\n",
    "#Get list of markers that I am subsetting\n",
    "df_cluster.columns = df_cluster.columns.str.split(':').str[-1].tolist()\n",
    "df_marker = df_cluster.loc[:,'CD90':'FOXP3']\n",
    "markerlist = df_marker.columns.tolist()\n",
    "\n",
    "#Select only the data for working markers\n",
    "df = df.loc[:,markerlist]\n",
    "\n",
    "#Select only SampleID you want to examine and only channels for normalization\n",
    "#array = ['BALBc-3', 'BALBc-2', 'BALBc-1']\n",
    "#df = df[df.sampleID.isin(array)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine which channels to include\n",
    "first = 0 #Number of channel where markers begin\n",
    "last = 45 #Last number of channel+1 since non-inclusive\n",
    "colnum = last-first #Number of total channels\n",
    "\n",
    "#Select columns with only your data\n",
    "dfz = df.iloc[:,first:last]\n",
    "\n",
    "#Save remaining columns in dataframe\n",
    "dfe = df.iloc[:, np.r_[:first, last:len(df.columns)]]\n",
    "\n",
    "#zscore of the column markers\n",
    "dfz1 = pd.DataFrame(zscore(dfz,0),index = dfz.index,columns = ['z0_'+i for i in dfz.columns])\n",
    "\n",
    "#zscore along rows\n",
    "dfz2 = pd.DataFrame(zscore(dfz1,1),index = dfz1.index,columns = ['z01_'+i for i in dfz.columns])\n",
    "\n",
    "#Take cumulative density function to find probability of z score across a row\n",
    "dfp = pd.DataFrame(norm.cdf(dfz2),index = dfz2.index,columns = ['p_'+i for i in dfz.columns])\n",
    "\n",
    "#First 1-probability and then take negative logarithm so greater values demonstrate positive cell type\n",
    "dflog = dfp.apply(lambda x: -np.log(1-x))\n",
    "dflog.columns = [i for i in dfz.columns]\n",
    "dflog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Use universal threshold accross channels\n",
    "#threshold = 4.6\n",
    "\n",
    "#2 Component Mixture Model to threshold the plog-values based on \n",
    "models = {}\n",
    "neg_idx = {}\n",
    "sigma = {}\n",
    "mu = {}\n",
    "gmm_thresh = {}\n",
    "\n",
    "gmm = GMM(n_components=2, n_init=30)\n",
    "\n",
    "for col in dflog.columns:\n",
    "    models[col] =  gmm.fit(dflog[col].to_numpy().reshape(-1, 1)) \n",
    "    \n",
    "    neg_idx[col] = np.argmax([models[col].means_])\n",
    "\n",
    "    sigma[col] = np.sqrt(models[col].covariances_[neg_idx[col]])\n",
    "\n",
    "    mu[col] = models[col].means_[neg_idx[col],0]\n",
    "    \n",
    "    gmm_thresh[col] = mu[col]+1*sigma[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace logarithmic of probability values less than threshold 3 deviations above mean of second mixture model, very little background\n",
    "\n",
    "dflogcopy = dflog\n",
    "\n",
    "for col in dflog.columns:\n",
    "    dflog[col] = (dflog[col] > gmm_thresh[col]).astype(int)\n",
    "\n",
    "dfr = dflog\n",
    "dfr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Unique Cell Types by Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a unique cell-description to each cell in dataframe and add column with clusterid\n",
    "dfr['combo'] = dfr.apply(lambda x: '+ '.join(list(x[x == 1].index)), axis=1)\n",
    "unique = list(dfr['combo'].unique())\n",
    "dfr['clusterid'] = [unique.index(i) for i in dfr['combo']]\n",
    "\n",
    "#Concatenate old values to Dataframe\n",
    "dfcon = pd.concat([dfr,df_loc],1)\n",
    "dfcon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataframe as pickle file\n",
    "os.chdir(\"/home/idies/workspace/Storage/jwhickey/persistent\")\n",
    "dfr.to_pickle('dfr_gaus')\n",
    "\n",
    "#Save interesting columns into csv file\n",
    "dfexport = dfcon.iloc[:,colnum:len(dfcon.columns)]\n",
    "dfexport.to_csv('data_tranformed_gaus.csv')\n",
    "\n",
    "#If you want the whole dataframe\n",
    "#####dfcon.to_csv('all_data_transformed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Unique Cell Combinations in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting list of column names and then counting unique combinations in dataframe\n",
    "npcol = dfr.columns.values\n",
    "nplist = npcol.tolist()\n",
    "dfcount = dfcon.groupby(nplist).size().to_frame('count').reset_index()\n",
    "\n",
    "#Sorting the Counts and then thresholding only unique combinations with greater than Cutoff counts\n",
    "Cutoff = 100\n",
    "dfsort = dfcount.sort_values(by ='count' , ascending=False).loc[dfcount['count'] >= Cutoff]\n",
    "dfsort.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Cells identified\n",
    "numcells = len(dfr.index)\n",
    "identcells = dfsort['count'].sum()\n",
    "percent = identcells/numcells\n",
    "print(\"Total Number of Cells = \" +str(numcells),\"Total Identified Cells from thresholding = \" +str(identcells), \n",
    "      \"Percent Identified Cells from thresholding = \" +str(percent), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See top 30 clusterid values of interest\n",
    "topnum = 30\n",
    "\n",
    "#Create dataframe for viewing interesting parameters\n",
    "cluster = dfsort.clusterid.values[:topnum]\n",
    "combo = dfsort.combo.values[:topnum]\n",
    "count = dfsort['count'].values[:topnum]\n",
    "percent_thres = dfsort['count'].values[:topnum]/identcells*100\n",
    "dftop = pd.DataFrame({'clusterid':cluster,'unique_combo':combo, '%of_ident': percent_thres, 'total_count':count})\n",
    "\n",
    "#export dataframe for looking at clusters elsewhere\n",
    "#####dftop.to_csv('top_celltypes.csv')\n",
    "\n",
    "dftop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Violin Plots for all of the data\n",
    "meltz = dflog.melt().sample(n=5000)\n",
    "\n",
    "#Set Plotting Style and Plot values as violin plots\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)}, font_scale=3.5)\n",
    "sns.catplot(data = meltz,col= 'variable',height = 15,y = 'value',kind = 'violin',\n",
    "            col_wrap =5,sharey = False,alpha = .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot the unique combinations as heatmap\n",
    "\n",
    "#Change the style of the plot\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)}, font_scale=1.0)\n",
    "\n",
    "#Change the dataframe to be averages by clusterID and do not plot more than number of columns\n",
    "sns.heatmap(dfsort.iloc[:,:colnum])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon)",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
